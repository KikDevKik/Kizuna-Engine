# Kizuna Engine: An√°lisis de Arquitectura y Visi√≥n

Este documento analiza el estado actual de Kizuna Engine, identifica problemas cr√≠ticos en la implementaci√≥n y propone la arquitectura ideal para cumplir con la visi√≥n de "Motor de Encarnaci√≥n Universal".

## 1. Arquitectura Actual (Estado Actual)

La arquitectura actual est√° dise√±ada como un sistema de streaming de audio full-duplex utilizando WebSockets para conectar un frontend React con un backend FastAPI que orquesta la API de Gemini Live.

### Backend (`backend/app/`)
*   **Tecnolog√≠a:** Python, FastAPI, Uvicorn, `google-genai` SDK.
*   **Flujo de Datos:**
    1.  **Recepci√≥n (Client -> Gemini):** Recibe audio PCM (16kHz, 16-bit, mono) a trav√©s de WebSocket.
    2.  **Buffering:** Implementa un buffer inteligente de ~100ms (3200 bytes) antes de enviar a Gemini. Esto es crucial para balancear latencia y carga de red, evitando saturar la API con paquetes diminutos.
    3.  **Env√≠o (Gemini -> Client):** Recibe chunks de audio y texto de Gemini en tiempo real y los reenv√≠a al cliente mediante un protocolo JSON personalizado (`{'type': 'audio', ...}`, `{'type': 'turn_complete'}`).
*   **Gesti√≥n de Conexi√≥n:** Utiliza `asyncio.TaskGroup` para manejar tareas de env√≠o y recepci√≥n simult√°neamente, asegurando que la desconexi√≥n en un sentido cierre limpiamente ambos lados.
*   **Modelo:** Configurado para usar `gemini-2.5-flash-native-audio-preview-12-2025`.

### Frontend (`frontend/src/`)
*   **Tecnolog√≠a:** React, TypeScript, Vite.
*   **Captura de Audio:** Utiliza `AudioWorklet` (`pcm-processor.js`) para procesar audio crudo (PCM 16-bit) directamente en un hilo separado, evitando bloqueos en la UI.
*   **Gesti√≥n de Estado:** El hook `useLiveAPI` mantiene referencias persistentes (`useRef`) a los contextos de audio y sockets para evitar reconexiones innecesarias durante re-renderizados de React.
*   **Estrategia de Conexi√≥n:** Implementa una filosof√≠a de "Conexi√≥n Indestructible". No se desconecta autom√°ticamente ante errores o eventos `onclose` del socket, permitiendo reconexiones o manejo manual para preservar la "presencia" de la IA.

---

## 2. Historial de Problemas y Correcciones

### ‚úÖ SOLUCIONADO: El Bucle de Retroalimentaci√≥n de Audio (Feedback Loop)
Anteriormente, en `frontend/src/hooks/useLiveAPI.ts`, exist√≠a una conexi√≥n err√≥nea que conectaba el micr√≥fono directamente a los altavoces:
`source.connect(ctx.destination);`

**Estado Actual:**
El problema ha sido corregido. La l√≠nea problem√°tica fue eliminada, asegurando que el audio del micr√≥fono solo se env√≠e al `AudioWorklet` para su transmisi√≥n al backend, evitando el eco y el feedback infinito.

### üü† Limitaci√≥n: Ausencia de Memoria Epist√©mica
Actualmente, la "personalidad" de Kizuna reside √∫nicamente en una instrucci√≥n de sistema simple ("Eres Kizuna...").
*   **Problema:** Si la sesi√≥n se reinicia, Kizuna olvida todo. No hay persistencia de hechos sobre el usuario (ej. nombre de mascotas, preferencias).
*   **Impacto:** Rompe la ilusi√≥n de una relaci√≥n continua ("Isekai Inverso"). Se siente como un "NPC gen√©rico" en lugar de un compa√±ero √∫nico.

### üü° Limitaci√≥n: Unimodalidad (Solo Audio)
La implementaci√≥n actual solo transmite audio.
*   **Visi√≥n:** Kizuna debe "ver" el entorno para comentar sobre √©l (ropa, desorden, clima).
*   **Estado:** El c√≥digo de WebSocket y el procesador de Gemini est√°n preparados para texto y audio, pero no hay flujo de video implementado desde el cliente.

---

## 3. Arquitectura Propuesta (La Visi√≥n Kizuna)

Para lograr el "Motor de Encarnaci√≥n Universal", la arquitectura debe evolucionar hacia un sistema multimodal con memoria persistente.

### A. Flujo de Audio Full-Duplex (‚úÖ IMPLEMENTADO)
El sistema actual cumple con el objetivo de latencia total (boca-a-o√≠do) de 400ms-600ms.

1.  **Frontend (Microphone):** `Microphone` -> `AudioContext` -> `AudioWorklet` -> `WebSocket`. **(SIN conexi√≥n a `destination`)**.
2.  **Backend (Routing):** `WebSocket` -> `Buffer (100ms)` -> `Gemini Live Session`.
3.  **Frontend (Speaker):** `WebSocket` -> `Decode Base64` -> `AudioBufferSource` -> `AudioContext.destination`.

*Nota:* La capacidad de interrupci√≥n (Barge-in) es posible gracias a la arquitectura full-duplex.

### B. Sistema de Memoria Epist√©mica (Deep Memory)
Para que Kizuna recuerde "tienes un gato llamado Luna" entre sesiones:

1.  **Base de Datos Vectorial (RAG):** Implementar una base de datos (como Pinecone, Weaviate o incluso un JSON local para empezar) que almacene "hechos" extra√≠dos de conversaciones anteriores.
2.  **Inyecci√≥n de Contexto:** Al iniciar una sesi√≥n (`GeminiLiveService.connect`), consultar la base de datos por hechos relevantes e inyectarlos en el `system_instruction` o como un mensaje inicial invisible ("Recuerda: El usuario tiene un gato llamado Luna").
3.  **Extracci√≥n de Memorias:** Un proceso secundario (o un prompt espec√≠fico al final de la sesi√≥n) que analice la conversaci√≥n y extraiga nuevos hechos para guardarlos.

### C. Percepci√≥n Multimodal (Visi√≥n)
Para que Kizuna "vea":

1.  **Captura de Video:** El frontend debe capturar frames de video (ej. 1 frame cada 1-2 segundos) usando un `Canvas` oculto y `canvas.toDataURL()`.
2.  **Env√≠o por WebSocket:** Enviar estos frames como mensajes JSON (`{ type: "image", data: "base64..." }`) por el mismo WebSocket existente.
3.  **Integraci√≥n Backend:** El backend debe recibir estos mensajes y enviarlos a la sesi√≥n de Gemini Live usando `session.send(input={"data": image_bytes, "mime_type": "image/jpeg"})`.

### D. Conexi√≥n "Indestructible" (‚úÖ IMPLEMENTADO)
La l√≥gica actual asegura que la conexi√≥n **nunca** se cierra por iniciativa del servidor, salvo error fatal irrecuperable. La IA espera pacientemente en silencio (como una persona en la habitaci√≥n) hasta que el usuario decida interactuar o cerrar la "invocaci√≥n".
